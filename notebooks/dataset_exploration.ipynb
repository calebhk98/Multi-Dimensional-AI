{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Dataset Exploration\n",
				"\n",
				"Explore and visualize the Multi-Dimensional AI dataset.\n",
				"\n",
				"**Purpose:**\n",
				"- Load and inspect dataset samples\n",
				"- Visualize vision/audio/sensor data\n",
				"- Verify data quality and preprocessing\n",
				"- Analyze data distribution"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sys\n",
				"sys.path.append('..')\n",
				"\n",
				"import torch\n",
				"import matplotlib.pyplot as plt\n",
				"import numpy as np\n",
				"from pathlib import Path\n",
				"\n",
				"from src.data.multimodal_dataset import SyntheticMultiModalDataset\n",
				"from src.data.real_dataset import RealMultiModalDataset\n",
				"from torch.utils.data import DataLoader"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Load Dataset"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load synthetic dataset for exploration\n",
				"dataset = SyntheticMultiModalDataset(\n",
				"\tnum_samples=100,\n",
				"\tseq_length=64,\n",
				"\tvocab_size=1000\n",
				")\n",
				"\n",
				"print(f\"Dataset size: {len(dataset)}\")\n",
				"print(f\"Sample 0 keys: {dataset[0].keys()}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Inspect Sample Data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get a sample\n",
				"sample = dataset[0]\n",
				"\n",
				"print(\"Input shapes:\")\n",
				"for key, value in sample['inputs'].items():\n",
				"\tif isinstance(value, torch.Tensor):\n",
				"\t\tprint(f\"  {key}: {value.shape}\")\n",
				"\n",
				"print(\"\\nTarget shapes:\")\n",
				"for key, value in sample['targets'].items():\n",
				"\tif isinstance(value, torch.Tensor):\n",
				"\t\tprint(f\"  {key}: {value.shape}\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Visualize Vision Input"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualize left eye image if available\n",
				"if 'vision_left' in sample['inputs']:\n",
				"\timg = sample['inputs']['vision_left']\n",
				"\t\n",
				"\t# Convert from CHW to HWC for visualization\n",
				"\tif img.dim() == 3:\n",
				"\t\timg_np = img.permute(1, 2, 0).numpy()\n",
				"\t\t\n",
				"\t\tplt.figure(figsize=(8, 8))\n",
				"\t\tplt.imshow(img_np)\n",
				"\t\tplt.title('Left Eye Vision Input')\n",
				"\t\tplt.axis('off')\n",
				"\t\tplt.show()\n",
				"else:\n",
				"\tprint(\"No vision data in this sample\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Visualize Audio Waveform"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Visualize audio waveform if available\n",
				"if 'audio' in sample['inputs']:\n",
				"\taudio = sample['inputs']['audio'].numpy()\n",
				"\t\n",
				"\tplt.figure(figsize=(12, 4))\n",
				"\tplt.plot(audio)\n",
				"\tplt.title('Audio Input Waveform')\n",
				"\tplt.xlabel('Sample')\n",
				"\tplt.ylabel('Amplitude')\n",
				"\tplt.grid(True)\n",
				"\tplt.show()\n",
				"else:\n",
				"\tprint(\"No audio data in this sample\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Analyze Sensor Data Distribution"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Collect touch sensor values across dataset\n",
				"touch_values = []\n",
				"for i in range(min(len(dataset), 50)):  # Sample first 50\n",
				"\tsample = dataset[i]\n",
				"\tif 'touch' in sample['inputs']:\n",
				"\t\ttouch_values.append(sample['inputs']['touch'].numpy())\n",
				"\n",
				"if touch_values:\n",
				"\ttouch_array = np.array(touch_values)\n",
				"\t\n",
				"\tplt.figure(figsize=(10, 6))\n",
				"\tfor finger in range(touch_array.shape[1]):\n",
				"\t\tplt.hist(touch_array[:, finger], alpha=0.5, label=f'Finger {finger}')\n",
				"\t\n",
				"\tplt.xlabel('Touch Value')\n",
				"\tplt.ylabel('Frequency')\n",
				"\tplt.title('Touch Sensor Value Distribution')\n",
				"\tplt.legend()\n",
				"\tplt.grid(True)\n",
				"\tplt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Data Validation"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Validate data ranges\n",
				"from src.data.validation import validate_input_shapes, validate_value_ranges\n",
				"\n",
				"try:\n",
				"\tvalidate_input_shapes(sample['inputs'])\n",
				"\tprint(\"✓ Input shapes are valid\")\n",
				"except ValueError as e:\n",
				"\tprint(f\"✗ Shape validation failed: {e}\")\n",
				"\n",
				"try:\n",
				"\tvalidate_value_ranges(sample['inputs'])\n",
				"\tprint(\"✓ Value ranges are valid\")\n",
				"except ValueError as e:\n",
				"\tprint(f\"✗ Range validation failed: {e}\")"
			]
		}
	],
	"metadata": {
		"kernelnel": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.0"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
