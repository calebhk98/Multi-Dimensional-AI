# Architecture Guide

## Overview

The Multi-Dimensional AI Creature uses a novel multi-modal transformer architecture designed to process 6 sensory streams simultaneously and generate 4 parallel output streams. This architecture allows the creature to exist autonomously within a VR world, perceiving and interacting with its environment in a human-like manner.

## Core Components

### 1. Unified Transformer

The heart of the system is a unified transformer model (initially 1B parameters, scaling to 7B). Unlike traditional multi-modal models that might separate processing for different modalities, this model integrates all inputs into a single shared latent space.

- **Architecture Type**: Decoder-only Transformer (like GPT) but with multi-modal heads.
- **Context Window**: Efficient handling of continuous streams (likely using sliding windows or memory mechanisms).

The heart of the system is a unified transformer model.

- **Fusion Strategy (Example)**: Fully Connected First Layer.
    - All encoder outputs are concatenated.
    - _Example scenario_: If Vision is 1000 units and Hearing is 100 units, the first hidden layer (e.g., 1000 units) receives inputs from ALL input modalities simultaneously.
    - **Note**: All unit counts listed below are _examples_ to illustrate the data flow.
- **Fan-Out Strategy**:
    - The last hidden layer connects to ALL decoders simultaneously.

### 2. Input Modalities (Encoders)

The creature processes 6 distinct sensory inputs. Each input is processed by a specific encoder before being fed into the main transformer.

1.  **Vision (Stereo)**:
    - **Input**: Left and Right eye camera feeds.
    - **Encoder**: CNN or ViT based encoder to extract visual features and depth cues.
    - **Future**: Could be raw pixels, or visual features, or 3D mesh data.
2.  **Hearing**:
    - **Input**: Audio stream (spectrogram/waveform).
    - **Encoder**: Audio Encoder (e.g., Whisper-like, spectrogram analysis).
    - **Scope**: Raw audio perception including speech tonality, music, sarcasm tone, environmental crashes, etc. This is the "ears" of the AI.
3.  **Touch**:
    - **Input**: Haptic feedback data from the VR environment (collision, texture).
    - **Encoder**: Specialized encoder for tactile sensor arrays (collision, texture, temp).
4.  **Proprioception**:
    - **Input**: Internal state of body joint angles, velocity, and position..
    - **Encoder**: Vector encoder for kinematic data (joint angles, velocity).
5.  **Internal Thoughts**:
    - **Input**: The "inner monologue" (text tokens) fed back from the previous step.
    - **Encoder**: Text Embedding Layer.
6.  **External Text** (Input Speech):
    - **Input**: Written words or transcribed text.
    - **Encoder**: Text Embedding Layer.
    - **Scope**: Words written by others (e.g., chat messages, books). This is distinct from "Hearing" which processes audio features.

### 3. Output Modalities (Decoders)

The model generates parallel streams at every time step.

1.  **Internal Thoughts**:
    - **Output**: Text tokens representing the creature's reasoning and planning.
    - **Decoder**: Text Head.
2.  **External Speech**:
    - **Output**: Generated words/text content.
    - **Note**: This is NOT TTS audio. This is the semantic content of what the AI wants to say. So this is like the text that gets generated by a LLM.
    - **Decoder**: Text Head.
3.  **Vocalizations**:
    - **Output**: Sound generation control.
    - **Scope**: Includes words (speech production) AND non-speech sounds (gasps, laughter, sighs).
    - **Decoder**: Audio/Token Head.
4.  **Body Control**:
    - **Output**: Direct muscle control signals.
    - **Note**: NOT high-level animation targets (like "wave hand"). The AI must control individual "muscles" to achieve movement.
    - **Decoder**: Continuous Value Predictor vs Discrete Action Head.

## Data Flow & Fusion

1.  **Sensory Acquisition**: VR environment captures raw data (images, audio, physics).
2.  **Encoding**: Raw data is converted into embeddings.
3.  **Fusion**: Embeddings from all modailties are concatenated or interleaved.
4.  **Processing**: The Transformer processes the fused sequence.
5.  **Generation**: The model predicts the next token/value for all 4 output streams simultaneously.
6.  **Actuation**: Decoders convert outputs into actions in the VR world (movement, sound).

## Parallel Token Generation

A key feature is the ability to generate tokens for Thoughts, Speech, Vocalizations, and Body Control in parallel. This ensures the creature can think while speaking and moving, creating a fluid and lifelike presence.

1.  **Inputs**: All Encoders -> Concatenated Vector.
2.  **Layer 1**: Fully Connected to Hidden Units.
    - Every node in Layer 1 sees inputs from ALL active modalities.
3.  **Transformer Processing**: Deep layers process the fused representation.
4.  **Last Layer**: Fully Connected to All Decoders.
5.  **Outputs**: All Decoders generate tokens/values simultaneously.
