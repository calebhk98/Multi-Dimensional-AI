# Training Configuration

training:
  phase: "backprop"  # Options: backprop, evolutionary
  
  # Optimization
  optimizer:
    type: "adamw"
    lr: 3.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.01
    eps: 1.0e-8
  
  # Learning rate schedule
  lr_scheduler:
    type: "cosine_with_warmup"
    warmup_steps: 10000
    max_steps: 1000000
    min_lr: 3.0e-5
  
  # Batch configuration
  batch_size: 16
  gradient_accumulation_steps: 4  # Effective batch = 64
  max_grad_norm: 1.0
  
  # Training steps
  max_steps: 1000000
  eval_interval: 5000
  save_interval: 10000
  log_interval: 100
  
  # Mixed precision
  mixed_precision: true
  precision_type: "bf16"  # Options: fp16, bf16
  
  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: 1
    
  # DeepSpeed configuration
  deepspeed:
    enabled: false
    config_path: "configs/deepspeed_config.json"

# Data configuration
data:
  synthetic_data_dir: "./data/synthetic"
  cache_dir: "./data/cache"
  
  # Data loading
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  
  # Data augmentation
  augmentation:
    audio_noise: 0.01
    visual_brightness: 0.1
    visual_contrast: 0.1

# Checkpointing
checkpointing:
  save_dir: "./checkpoints"
  keep_last_n: 5
  save_optimizer_state: true
  save_on_error: true

# Logging
logging:
  log_dir: "./logs"
  use_wandb: true
  wandb_project: "multi-dimensional-ai"
  wandb_entity: "your_username"
  log_gradients: false
  log_learning_rate: true
  log_memory: true

# Validation
validation:
  enabled: true
  val_split: 0.05
  max_val_steps: 100
  metrics:
    - "perplexity"
    - "token_accuracy"
    - "cross_modal_consistency"
