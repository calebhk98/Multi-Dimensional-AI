# Configuration for Single-Modality Training (Refactored)
# Used by scripts/train_single_modality.py

defaults:
    seed: 42
    device: "cuda" # or "cpu"
    save_dir: "checkpoints/single_modality"

# Common training parameters
training:
    max_steps: 1000
    log_interval: 10
    save_interval: 200
    loss_weights:
        internal_text: 1.0
        external_text: 1.0
        audio: 1.0
        animation: 1.0
    optimizer:
        lr: 3.0e-4
        weight_decay: 0.01
        betas: [0.9, 0.95]

# Model Architecture Configuration (Shared across all)
model:
    transformer:
        num_layers: 4 # Reduced for single-modality testing/training speed
        hidden_dim: 768
        num_attention_heads: 12
        ffn_dim: 3072
        dropout: 0.1
    fusion:
        strategy: "concatenate"
        modality_embeddings: true
    encoders:
        internal_voice:
            vocab_size: 50257
        audio:
            sample_rate: 16000
            num_conv_layers: 4 # Reduced defaults
            conv_channels: 256
            codebook_size: 1024
        vision:
            image_size: 224
            patch_size: 16
        proprioception:
            num_joints: 24
            temporal_window: 10
        touch:
            num_contact_points: 10
    decoders:
        audio:
            codebook_size: 1024
        animation:
            num_joints: 24

# Modality-specific overrides (mostly for dataset generation parameters)
modalities:
    audio:
        batch_size: 4
        sample_rate: 16000
        audio_duration: 1.0
        embedding_dim: 768 # Match model.transformer.hidden_dim
        codebook_size: 1024

    voice_internal:
        batch_size: 8
        vocab_size: 50257
        max_seq_length: 64
        embedding_dim: 768

    voice_external:
        batch_size: 8
        vocab_size: 50257
        max_seq_length: 64
        embedding_dim: 768

    motion:
        batch_size: 16
        num_joints: 24
        temporal_window: 10
        embedding_dim: 768
