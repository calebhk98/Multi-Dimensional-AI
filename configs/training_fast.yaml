# Fast Training Configuration - Optimized for Speed
# Use with: python scripts/train_text_fast.py --config configs/training_fast.yaml

training:
    phase: "backprop"

    # Optimization
    optimizer:
        type: "adamw"
        lr: 6.0e-4
        betas: [0.9, 0.95]
        weight_decay: 0.1
        eps: 1.0e-8

    # Learning rate schedule
    scheduler:
        type: "cosine"
        warmup_steps: 1000
        min_lr: 6.0e-5

    # Batch configuration - tune for your GPU
    # RTX 3090 (24GB): batch_size 8-16, seq_length 1024
    # RTX 4090 (24GB): batch_size 12-20, seq_length 1024
    batch_size: 8
    gradient_accumulation_steps: 8  # Effective batch = 64
    max_grad_norm: 1.0

    # PERFORMANCE OPTIMIZATIONS
    # Use FP16 instead of BF16 on RTX 3090 (better tensor core support)
    mixed_precision: "fp16"

    # DISABLE these for speed on models < 500M params
    compile_model: false          # torch.compile can be slow to warmup
    enable_flash_attention: true  # Keep this on
    gradient_checkpointing: false # Only needed for very large models

    # Training steps
    max_steps: 100000
    eval_interval: 1000
    save_interval: 5000
    log_interval: 10

    # Curriculum Learning - start small for faster early iterations
    curriculum:
        enabled: false  # Disable for consistent benchmarking

# Data configuration
data:
    # CRITICAL: num_workers > 0 for parallel data loading
    num_workers: 4
    prefetch_factor: 2
    pin_memory: true

# Checkpointing
checkpointing:
    save_dir: "./checkpoints/fast"
    keep_last_n: 5
    save_optimizer_state: true

# Logging
logging:
    log_dir: "./logs"
    use_wandb: false  # Disable for faster iterations
    log_learning_rate: true
    log_memory: true
